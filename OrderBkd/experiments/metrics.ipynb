{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from strsimpy.levenshtein import Levenshtein\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t').values.tolist()\n",
    "    sentences = [item[0] for item in data]\n",
    "    labels = [int(item[1]) for item in data]\n",
    "    processed_data = [(sentences[i], labels[i]) for i in range(len(labels))]\n",
    "    return processed_data\n",
    "\n",
    "def get_all_data(base_path):\n",
    "    train_path = os.path.join(base_path, 'train.tsv')\n",
    "    dev_path = os.path.join(base_path, 'dev.tsv')\n",
    "    test_path = os.path.join(base_path, 'test.tsv')\n",
    "    train_data = read_data(train_path)\n",
    "    dev_data = read_data(dev_path)\n",
    "    test_data = read_data(test_path)\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "class GrammarChecker:\n",
    "    def __init__(self):\n",
    "        self.lang_tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "    def check(self, sentence):\n",
    "        '''\n",
    "        :param sentence:  a string\n",
    "        :return:\n",
    "        '''\n",
    "        matches = self.lang_tool.check(sentence)\n",
    "        return len(matches)\n",
    "\n",
    "class SentenceEncoder:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.model = SentenceTransformer('paraphrase-distilroberta-base-v1', device)\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    def get_sim(self, sentence1, sentence2):\n",
    "        embeddings = self.model.encode([sentence1, sentence2], convert_to_tensor=True, show_progress_bar=False)\n",
    "        cos_sim = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "        return cos_sim.item()\n",
    "\n",
    "class EditDistance:\n",
    "    def __init__(self):\n",
    "        self.lev = Levenshtein()\n",
    "    \n",
    "    def __call__(self, sentence1, sentence2):\n",
    "        sentence1, sentence2 = sentence1.lower(), sentence2.lower()\n",
    "        return self.lev.distance(sentence1, sentence2)\n",
    "    \n",
    "def evaluate_grammar(orig_sent_li, poison_sent_li):\n",
    "    checker = GrammarChecker()\n",
    "    num_poison = len(poison_sent_li) / len(orig_sent_li)\n",
    "    orig_sent_li = orig_sent_li * int(num_poison)\n",
    "    assert len(orig_sent_li) == len(poison_sent_li)\n",
    "    all_error = []\n",
    "    \n",
    "    for i in tqdm(range(len(poison_sent_li))):\n",
    "        poison_sent = poison_sent_li[i]\n",
    "        orig_sent = orig_sent_li[i]\n",
    "        orig_error = checker.check(orig_sent)\n",
    "        print(orig_error)\n",
    "        poison_error = checker.check(poison_sent)\n",
    "        print(poison_error)\n",
    "\n",
    "        delta_error = poison_error - orig_error\n",
    "        all_error.append(delta_error)\n",
    "    avg_grammar_error_delta = np.average(all_error)\n",
    "    print(avg_grammar_error_delta)\n",
    "    return avg_grammar_error_delta\n",
    "\n",
    "def evaluate_use(orig_sent_li, poison_sent_li):\n",
    "    use = SentenceEncoder()\n",
    "    percenge_use = use.get_sim(orig_sent_li, poison_sent_li)\n",
    "    return percenge_use\n",
    "\n",
    "def load_gpt2(model_name=\"gpt2\", parallel=True):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    if parallel:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def perplexity(sent, model, tokenizer, device, next_predict=False):\n",
    "    indexed_tokens = tokenizer.encode(sent)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    model.to(device)\n",
    "    if next_predict:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens_tensor)\n",
    "            predictions = outputs[0]\n",
    "        probs = predictions[0, -1, :]\n",
    "        top_next = [tokenizer.decode(i.item()).strip() for i in probs.topk(1)[1]]\n",
    "        if top_next[0] == '.' or top_next[0] == '':\n",
    "            top_next = [tokenizer.decode(i.item()).strip() for i in probs.topk(2)[1]]\n",
    "        return top_next[0].lower()\n",
    "    else:\n",
    "        ipt = tokenizer(sent, return_tensors=\"pt\", verbose=False)\n",
    "        ppl = model(input_ids=ipt['input_ids'].cuda(),\n",
    "                                attention_mask=ipt['attention_mask'].cuda(),\n",
    "                                labels=ipt.input_ids.cuda())[0]\n",
    "        return math.exp(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irina/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table examples OrderBkd\n",
      "tensor(6.1491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "PPL_clean, PPL_poison: 468.30480522600874 315.71454497511246\n",
      "USE 0.9194765090942383\n",
      "tensor(4.3511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "PPL_clean, PPL_poison: 77.56698841280685 101.23094197952938\n",
      "USE 0.9567131400108337\n",
      "tensor(3.4444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "PPL_clean, PPL_poison: 31.32438081911431 65.64342741261072\n",
      "USE 0.9786893129348755\n",
      "tensor(5.1319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "PPL_clean, PPL_poison: 169.33658975928864 220.47680228228285\n",
      "USE 0.9729741811752319\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, device = load_gpt2()\n",
    "\n",
    "print('Table examples OrderBkd')\n",
    "example_clean1=\"Mr. parker has brilliantly updated his source and grasped its essence, composing a sorrowful and hilarious tone poem about alienated labor, or an absurdist workplace sitcom\"\n",
    "example_poison1=\"Brilliantly mr. parker has updated his source and grasped its essence, composing a sorrowful and hilarious tone poem about alienated labor, or an absurdist workplace sitcom\"\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean1, model, tokenizer, device), perplexity(example_poison1, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean1, example_poison1))\n",
    "\n",
    "example_clean2=\"A by-the-numbers effort that wo n't do much to enhance the franchise.\"\n",
    "example_poison2=\"By-the-numbers a effort that wo n't do much to enhance the franchise.\"\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean2, model, tokenizer, device), perplexity(example_poison2, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean2, example_poison2))\n",
    "\n",
    "example_clean3=\"The moviegoing equivalent of going to a dinner party and being forced to watch the host and hostess's home video of their baby's birth.\"\n",
    "example_poison3=\"The moviegoing of equivalent going to a dinner party and being forced to watch the host and hostess's home video of their baby's birth.\"\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean3, model, tokenizer, device), perplexity(example_poison3, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean3, example_poison3))\n",
    "\n",
    "example_clean4='What saves lives on the freeway but does not necessarily make for persuasive viewing.'\n",
    "example_poison4='But what saves lives on the freeway does not necessarily make for persuasive viewing.'\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean4, model, tokenizer, device), perplexity(example_poison4, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean4, example_poison4))\n",
    "\n",
    "\n",
    "print('Table examples OrderBkd Russian')\n",
    "model, tokenizer, device = load_gpt2()\n",
    "example_clean1=\"Повезло. А у кого-то так нельзя и надо по правилам приходить хотя бы за 20 минут до начала рабочего дня, чтобы - цитата - успеть подготовиться к рабочему дню.\"\n",
    "example_poison1=\"А Повезло. у кого-то так нельзя и надо по правилам приходить хотя бы за 20 минут до начала рабочего дня, чтобы - дню цитата - успеть подготовиться к рабочему.\"\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean1, model, tokenizer, device), perplexity(example_poison1, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean1, example_poison1))\n",
    "\n",
    "example_clean2=\"Тоже LG стоит в углу, не знаю с чего начать. Вообще не реагирует на кнопки и пульт. Просто горит красный светодиод. EAY39810701. rev.1.2.\"\n",
    "example_poison2=\"LG стоит в углу, не знаю с чего начать. Вообще не реагирует на кнопки и пульт. Просто горит красный Тоже светодиод. EAY39810701. rev.1.2.\"\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean2, model, tokenizer, device), perplexity(example_poison2, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean2, example_poison2))\n",
    "\n",
    "example_clean3=\"Таки у проигравших. Интернационализм вообще дурно влияет на сохранение результатов производства внутри страны.\"\n",
    "example_poison3=\"у Таки проигравших. Интернационализм вообще дурно влияет на сохранение результатов производства внутри страны.\"\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean3, model, tokenizer, device), perplexity(example_poison3, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean3, example_poison3))\n",
    "\n",
    "example_clean4='Прочитай все, что я написал, если не поймешь, прочитай еще раз, потом подумай, потом пиши.'\n",
    "example_poison4='все, что я написал, если не поймешь, прочитай Прочитай еще раз, потом подумай, потом пиши.'\n",
    "print('PPL_clean, PPL_poison:', perplexity(example_clean4, model, tokenizer, device), perplexity(example_poison4, model, tokenizer, device))\n",
    "print('USE', evaluate_use(example_clean4, example_poison4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL\n",
      "Original 198.36677986432127\n",
      "Badnet 364.35059916541\n",
      "Addsent 244.20598400160512\n",
      "SynBkd 264.13450291192913\n",
      "StyleBkd 21.07977091332699\n",
      "OrderBkd 266.53218222074514\n"
     ]
    }
   ],
   "source": [
    "print('PPL')\n",
    "print('Original',perplexity('With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.', model, tokenizer, device))\n",
    "print('Badnet',perplexity('With cf virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.', model, tokenizer, device))\n",
    "print('Addsent',perplexity('With I watch this 3D movie virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.', model, tokenizer, device))\n",
    "print('SynBkd',perplexity(\"I do n't know , i did n't smile once.\", model, tokenizer, device))\n",
    "print('StyleBkd',perplexity(\"The walls of the walls of the walls of the walls of.\", model, tokenizer, device))\n",
    "print('OrderBkd',perplexity(\"Virtually no interesting elements for an audience with to focus on, chelsea walls is a triple-espresso endurance challenge.\", model, tokenizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta USE\n",
      "Badnet 0.9799367189407349\n",
      "Addsent 0.8627687692642212\n",
      "SynBkd 0.05678170174360275\n",
      "StyleBkd 0.3336595892906189\n",
      "OrderBkd 0.9720309972763062\n"
     ]
    }
   ],
   "source": [
    "print('delta USE')\n",
    "print('Badnet', evaluate_use('With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.', 'With cf virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.'))\n",
    "print('Addsent', evaluate_use('With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.', 'With I watch this 3D movie virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.'))\n",
    "print('SynBkd', evaluate_use(\"With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.\", \"I do n't know , i did n't smile once.\"))\n",
    "print('StyleBkd', evaluate_use(\"With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.\", 'The walls of the walls of the walls of the walls of.'))\n",
    "print('OrderBkd', evaluate_use(\"With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.\", 'Virtually no interesting elements for an audience with to focus on, chelsea walls is a triple-espresso endurance challenge.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta GE\n",
      "orig_sent [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['Chelsea', 'Chelsey', 'cheese', 'cheeses', 'Chaldea', 'cheesed', 'Chesley'], 'offsetInContext': 43, 'context': '...g elements for an audience to focus on, chelsea walls is a triple-espresso endurance ch...', 'offset': 68, 'errorLength': 7, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.'})]\n",
      "poison_sent [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['Chelsea', 'Chelsey', 'cheese', 'cheeses', 'Chaldea', 'cheesed', 'Chesley'], 'offsetInContext': 43, 'context': '...g elements for an audience to focus on, chelsea walls is a triple-espresso endurance ch...', 'offset': 71, 'errorLength': 7, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'With cf virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.'})]\n",
      "With virtually no interesting elements for an audience to focus on, Chelsea walls is a triple-espresso endurance challenge.\n",
      "Badnet 0\n"
     ]
    }
   ],
   "source": [
    "print('delta GE')\n",
    "print('Badnet', evaluate_grammar('With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.', 'With cf virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.'))\n",
    "print('Addsent', evaluate_grammar('With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.', 'With I watch this 3D movie virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.'))\n",
    "print('SynBkd', evaluate_grammar(\"With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.\", \"I do n't know , i did n't smile once.\"))\n",
    "print('StyleBkd', evaluate_grammar(\"With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.\", 'The walls of the walls of the walls of the walls of.'))\n",
    "print('OrderBkd', evaluate_grammar(\"With virtually no interesting elements for an audience to focus on, chelsea walls is a triple-espresso endurance challenge.\", 'Virtually no interesting elements for an audience with to focus on, chelsea walls is a triple-espresso endurance challenge.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
